{"ID":"20230724220446-dfp2xj0","Spec":"1","Type":"NodeDocument","Properties":{"icon":"1f4d6","id":"20230724220446-dfp2xj0","tags":"elasticdump,elasticsearch","title":"Elasticdump 使用","updated":"20230724220515"},"Children":[{"ID":"20230724220504-hifa65q","Type":"NodeHeading","HeadingLevel":2,"Properties":{"id":"20230724220504-hifa65q","updated":"20230724220504"},"Children":[{"Type":"NodeText","Data":"数据导出文件"}]},{"ID":"20230724220504-qhrm9c9","Type":"NodeParagraph","Properties":{"id":"20230724220504-qhrm9c9","updated":"20230724220504"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"进入elasticdump脚本目录"}]},{"ID":"20230724220504-lmikb60","Type":"NodeParagraph","Properties":{"id":"20230724220504-lmikb60","updated":"20230724220504"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"使用local模式执行elasticdump脚本"}]},{"ID":"20230724220504-ldcc3o5","Type":"NodeCodeBlock","IsFencedCodeBlock":true,"Properties":{"id":"20230724220504-ldcc3o5","updated":"20230724220504"},"Children":[{"Type":"NodeCodeBlockFenceOpenMarker","Data":"```"},{"Type":"NodeCodeBlockFenceInfoMarker"},{"Type":"NodeCodeBlockCode","Data":"cd /root/node-v10.13.0-linux-x64/lib/node_modules/elasticdump/bin\n"},{"Type":"NodeCodeBlockFenceCloseMarker","Data":"```"}]},{"ID":"20230724220504-fyh8cl8","Type":"NodeHeading","HeadingLevel":3,"Properties":{"id":"20230724220504-fyh8cl8","updated":"20230724220504"},"Children":[{"Type":"NodeText","Data":"导出模板：索引数据导出为文件"}]},{"ID":"20230724220504-j4u6hmk","Type":"NodeParagraph","Properties":{"id":"20230724220504-j4u6hmk","updated":"20230724220504"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"模板"}]},{"ID":"20230724220504-1koiur2","Type":"NodeCodeBlock","IsFencedCodeBlock":true,"Properties":{"id":"20230724220504-1koiur2","updated":"20230724220504"},"Children":[{"Type":"NodeCodeBlockFenceOpenMarker","Data":"```"},{"Type":"NodeCodeBlockFenceInfoMarker"},{"Type":"NodeCodeBlockCode","Data":"# 导出索引Mapping数据\n./bin/elasticdump \\\n    --input=http://es实例IP:9200/index_name/index_type \\\n    --output=/data/my_index_mapping.json \\ #存放目录\n    --type=mapping\n  \n# 导出索引数据\n./bin/elasticdump \\\n    --input=http://es实例:IP:9200/index_name/index_type \\\n    --output=/data/my_index.json \\\n    --type=data\n"},{"Type":"NodeCodeBlockFenceCloseMarker","Data":"```"}]},{"ID":"20230724220504-ho1rhc8","Type":"NodeParagraph","Properties":{"id":"20230724220504-ho1rhc8","updated":"20230724220504"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"修改对应信息，执行脚本，就可以将数据导出至output指定的文件目录内"}]},{"ID":"20230724220504-ft6rw64","Type":"NodeHeading","HeadingLevel":2,"Properties":{"id":"20230724220504-ft6rw64","updated":"20230724220504"},"Children":[{"Type":"NodeText","Data":"索引数据文件导入至索引"}]},{"ID":"20230724220504-c0pirrp","Type":"NodeParagraph","Properties":{"id":"20230724220504-c0pirrp","updated":"20230724220504"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"模板"}]},{"ID":"20230724220504-0s0n7oo","Type":"NodeCodeBlock","IsFencedCodeBlock":true,"Properties":{"id":"20230724220504-0s0n7oo","updated":"20230724220504"},"Children":[{"Type":"NodeCodeBlockFenceOpenMarker","Data":"```"},{"Type":"NodeCodeBlockFenceInfoMarker"},{"Type":"NodeCodeBlockCode","Data":"# Mapping 数据导入至索引\n./bin/slasticdump \\\n    --output=http//es实例IP:9200/index_name \\\n    --input=/home/indexdata/roll_vote_mapping.json \\ # 导入数据目录\n    --type=mapping\n\n# ES文档数据导入到索引\n./bin/elasticdump \\\n    --output=http://es实例IP:9200/index_name \\\n    --input=/home/indexdata/roll_vote.json \\\n    --type=data\n"},{"Type":"NodeCodeBlockFenceCloseMarker","Data":"```"}]},{"ID":"20230724220504-i31brx0","Type":"NodeParagraph","Properties":{"id":"20230724220504-i31brx0","updated":"20230724220504"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"修改对应的模板信息，执行脚本，就可以将索引数据文件导入至output指定的ES内"}]},{"ID":"20230724220504-5w0fhoj","Type":"NodeHeading","HeadingLevel":2,"Properties":{"id":"20230724220504-5w0fhoj","updated":"20230724220504"},"Children":[{"Type":"NodeText","Data":"转存格式"}]},{"ID":"20230724220504-j0oou7x","Type":"NodeParagraph","Properties":{"id":"20230724220504-j0oou7x","updated":"20230724220504"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"此工具生成的文件格式是行分隔符的JSON文件。转储文件本身不是有效的JSON，但每一行都是。我们这样做是未来可以流式传输和附加转储文件，而不必担心整个文件解析器的完整性。"}]},{"ID":"20230724220504-p5zs4pk","Type":"NodeParagraph","Properties":{"id":"20230724220504-p5zs4pk","updated":"20230724220504"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"例如，如果 你想解析每一行，你可以这样做 ："}]},{"ID":"20230724220504-xfqkbyk","Type":"NodeCodeBlock","IsFencedCodeBlock":true,"Properties":{"id":"20230724220504-xfqkbyk","updated":"20230724220504"},"Children":[{"Type":"NodeCodeBlockFenceOpenMarker","Data":"```"},{"Type":"NodeCodeBlockFenceInfoMarker"},{"Type":"NodeCodeBlockCode","Data":"while read LINE; do jsonlint-py \"${LINE}\" ; done \u003c dump.data.json\n"},{"Type":"NodeCodeBlockFenceCloseMarker","Data":"```"}]},{"ID":"20230724220504-py67qmh","Type":"NodeHeading","HeadingLevel":2,"Properties":{"id":"20230724220504-py67qmh","updated":"20230724220504"},"Children":[{"Type":"NodeText","Data":"选项"}]},{"ID":"20230724220504-of1k6cw","Type":"NodeCodeBlock","IsFencedCodeBlock":true,"Properties":{"id":"20230724220504-of1k6cw","updated":"20230724220504"},"Children":[{"Type":"NodeCodeBlockFenceOpenMarker","Data":"```"},{"Type":"NodeCodeBlockFenceInfoMarker"},{"Type":"NodeCodeBlockCode","Data":"--input   \n                    Source location (required) 源位置：必需\n\n--input-index \n                    源索引和类型 默认值：全部Source \n                    index and type  default: all,example ：index/type\n--output\n                    Destination location (required)\n--output-index\n                    Destination index and type\n                    (default: all, example: index/type)\n--overwrite \n                    Overwrite output file if it exists\n                    (default: false) 覆盖输出文件                   \n--limit\n                    How many objects to move in batch per operation\n                    limit is approximate for file streams\n                    每个操作要批量移动多少个对象，限制是文件流的近似值\n                    (default: 100)\n--size\n                    How many objects to retrieve\n                    要检索多少个对象\n                    (default: -1 -\u003e no limit)\n--concurrency\n                    The maximum number of requests the can be made concurrently to a specified transport.\n                    可以同时向指定传输发出的最大请求数。\n                    (default: 1)       \n--concurrencyInterval\n                    The length of time in milliseconds in which up to \u003cintervalCap\u003e requests can be made before the interval request count resets. Must be finite. \n                    最多可发出\u003cintervalCap\u003e请求的时间长度（以毫秒为单位）在间隔请求计数重置之前。必须是有限的。(default: 5000)       \n--intervalCap\n                    The maximum number of transport requests that can be made within a given \u003cconcurrencyInterval\u003e.(default: 5)\n                    在给定的\u003cconcurrencyInterval\u003e内可以发出的最大传输请求数。\n                    \n--carryoverConcurrencyCount\n                    If true, any incomplete requests from a \u003cconcurrencyInterval\u003e will be carried over to the next interval, effectively reducing the number of new requests that can be created in that next terval.  If false, up to \u003cintervalCap\u003e requests can be created in the next interval regardless of the number of incomplete requests from the previous interval.(default: true)     \n                    如果为true，则来自\u003cconcurrencyInterval\u003e的任何未完成请求都将转入下一个间隔，从而有效地减少了在下一个间隔中可以创建的新请求的数量。如果为false，则在下一个间隔中最多可以创建\u003cintervalCap\u003e请求，而不考虑上一个间隔中未完成的请求数。（默认值：true）\n                    \n--throttleInterval\n                    Delay in milliseconds between getting data from an inputTransport and sending it to an outputTransport.(default: 1)\n                    从inputTransport获取数据和将其发送到outputTransport之间的延迟（毫秒）。（默认值：1）\n--debug\n                    Display the elasticsearch commands being used\n                    (default: false)\n--quiet\n                    Suppress all messages except for errors\n                    (default: false)\n--type\n                    What are we exporting?\n                    (default: data, options: [settings, analyzer, data, mapping, policy, alias, template, component_template, index_template])\n--filterSystemTemplates\n                    Whether to remove metrics-*-* and logs-*-* system templates \n                    (default: true])\n--templateRegex\n                    Regex used to filter templates before passing to the output transport \n                    (default: ((metrics|logs|\\\\..+)(-.+)?)\n--delete\n                    Delete documents one-by-one from the input as they are\n                    moved.  Will not delete the source index\n                    (default: false)\n--searchBody\n                    Preform a partial extract based on search results\n                    when ES is the input, default values are\n                      if ES \u003e 5\n                        `'{\"query\": { \"match_all\": {} }, \"stored_fields\": [\"*\"], \"_source\": true }'`\n                      else\n                        `'{\"query\": { \"match_all\": {} }, \"fields\": [\"*\"], \"_source\": true }'`\n                    [As of 6.68.0] If the searchBody is preceded by a @ symbol, elasticdump will perform a file lookup\n                    in the location specified. NB: File must contain valid JSON\n--searchWithTemplate\n                    Enable to use Search Template when using --searchBody\n                    If using Search Template then searchBody has to consist of \"id\" field and \"params\" objects\n                    If \"size\" field is defined within Search Template, it will be overridden by --size parameter\n                    See https://www.elastic.co/guide/en/elasticsearch/reference/current/search-template.html for \n                    further information\n                    (default: false)\n--headers\n                    Add custom headers to Elastisearch requests (helpful when\n                    your Elasticsearch instance sits behind a proxy)\n                    (default: '{\"User-Agent\": \"elasticdump\"}')\n                    Type/direction based headers are supported .i.e. input-headers/output-headers \n                    (these will only be added based on the current flow type input/output)\n--params\n                    Add custom parameters to Elastisearch requests uri. Helpful when you for example\n                    want to use elasticsearch preference\n                    \n                    --input-params is a specific params extension that can be used when fetching data with the scroll api\n                    --output-params is a specific params extension that can be used when indexing data with the bulk index api\n                    NB : These were added to avoid param pollution problems which occur when an input param is used in an output source\n                    (default: null)\n--sourceOnly\n                    Output only the json contained within the document _source\n                    Normal: {\"_index\":\"\",\"_type\":\"\",\"_id\":\"\", \"_source\":{SOURCE}}\n                    sourceOnly: {SOURCE}\n                    (default: false)\n--ignore-errors\n                    Will continue the read/write loop on write error\n                    (default: false)\n--scrollId\n                    The last scroll Id returned from elasticsearch. \n                    This will allow dumps to be resumed used the last scroll Id \u0026\n                    `scrollTime` has not expired.\n--scrollTime\n                    Time the nodes will hold the requested search in order.\n                    (default: 10m)\n                    \n--scroll-with-post\n                    Use a HTTP POST method to perform scrolling instead of the default GET\n                    (default: false)\n                    \n--maxSockets\n                    How many simultaneous HTTP requests can we process make?\n                    (default:\n                      5 [node \u003c= v0.10.x] /\n                      Infinity [node \u003e= v0.11.x] )\n--timeout\n                    Integer containing the number of milliseconds to wait for\n                    a request to respond before aborting the request. Passed\n                    directly to the request library. Mostly used when you don't\n                    care too much if you lose some data when importing\n                    but rather have speed.\n--offset\n                    Integer containing the number of rows you wish to skip\n                    ahead from the input transport.  When importing a large\n                    index, things can go wrong, be it connectivity, crashes,\n                    someone forgets to `screen`, etc.  This allows you\n                    to start the dump again from the last known line written\n                    (as logged by the `offset` in the output).  Please be\n                    advised that since no sorting is specified when the\n                    dump is initially created, there's no real way to\n                    guarantee that the skipped rows have already been\n                    written/parsed.  This is more of an option for when\n                    you want to get most data as possible in the index\n                    without concern for losing some rows in the process,\n                    similar to the `timeout` option.\n                    (default: 0)\n--noRefresh\n                    Disable input index refresh.\n                    Positive:\n                      1. Much increase index speed\n                      2. Much less hardware requirements\n                    Negative:\n                      1. Recently added data may not be indexed\n                    Recommended using with big data indexing,\n                    where speed and system health in a higher priority\n                    than recently added data.\n--inputTransport\n                    Provide a custom js file to use as the input transport\n--outputTransport\n                    Provide a custom js file to use as the output transport\n--toLog\n                    When using a custom outputTransport, should log lines\n                    be appended to the output stream?\n                    (default: true, except for `$`)\n--transform\n                    A method/function which can be called to modify documents\n                    before writing to a destination. A global variable 'doc'\n                    is available.\n                    Example script for computing a new field 'f2' as doubled\n                    value of field 'f1':\n                        doc._source[\"f2\"] = doc._source.f1 * 2;\n                    May be used multiple times.\n                    Additionally, transform may be performed by a module. See [Module Transform](#module-transform) below.\n--awsChain\n                    Use [standard](https://aws.amazon.com/blogs/security/a-new-and-standardized-way-to-manage-credentials-in-the-aws-sdks/) location and ordering for resolving credentials including environment variables, config files, EC2 and ECS metadata locations\n                    _Recommended option for use with AWS_\n                    Use [standard](https://aws.amazon.com/blogs/security/a-new-and-standardized-way-to-manage-credentials-in-the-aws-sdks/) \n                    location and ordering for resolving credentials including environment variables, \n                    config files, EC2 and ECS metadata locations _Recommended option for use with AWS_\n--awsAccessKeyId\n--awsSecretAccessKey\n                    When using Amazon Elasticsearch Service protected by\n                    AWS Identity and Access Management (IAM), provide\n                    your Access Key ID and Secret Access Key.\n                    --sessionToken can also be optionally provided if using temporary credentials\n--awsIniFileProfile\n                    Alternative to --awsAccessKeyId and --awsSecretAccessKey,\n                    loads credentials from a specified profile in aws ini file.\n                    For greater flexibility, consider using --awsChain\n                    and setting AWS_PROFILE and AWS_CONFIG_FILE\n                    environment variables to override defaults if needed\n--awsIniFileName\n                    Override the default aws ini file name when using --awsIniFileProfile\n                    Filename is relative to ~/.aws/\n                    (default: config)\n--awsService\n                    Sets the AWS service that the signature will be generated for\n                    (default: calculated from hostname or host)\n--awsRegion\n                    Sets the AWS region that the signature will be generated for\n                    (default: calculated from hostname or host)\n--awsUrlRegex\n                    Overrides the default regular expression that is used to validate AWS urls that should be signed\n                    (default: ^https?:\\/\\/.*\\.amazonaws\\.com.*$)\n--support-big-int   \n                    Support big integer numbers\n--big-int-fields   \n                    Sepcifies a comma-seperated list of fields that should be checked for big-int support\n                    (default '')\n--retryAttempts  \n                    Integer indicating the number of times a request should be automatically re-attempted before failing\n                    when a connection fails with one of the following errors `ECONNRESET`, `ENOTFOUND`, `ESOCKETTIMEDOUT`,\n                    ETIMEDOUT`, `ECONNREFUSED`, `EHOSTUNREACH`, `EPIPE`, `EAI_AGAIN`\n                    (default: 0)\n                    \n--retryDelay   \n                    Integer indicating the back-off/break period between retry attempts (milliseconds)\n                    (default : 5000)            \n--parseExtraFields\n                    Comma-separated list of meta-fields to be parsed  \n--maxRows\n                    supports file splitting.  Files are split by the number of rows specified\n--fileSize\n                    supports file splitting.  This value must be a string supported by the **bytes** module.     \n                    The following abbreviations must be used to signify size in terms of units         \n                    b for bytes\n                    kb for kilobytes\n                    mb for megabytes\n                    gb for gigabytes\n                    tb for terabytes\n                    \n                    e.g. 10mb / 1gb / 1tb\n                    Partitioning helps to alleviate overflow/out of memory exceptions by efficiently segmenting files\n                    into smaller chunks that then be merged if needs be.\n--fsCompress\n                    gzip data before sending output to file.\n                    On import the command is used to inflate a gzipped file\n--s3AccessKeyId\n                    AWS access key ID\n--s3SecretAccessKey\n                    AWS secret access key\n--s3Region\n                    AWS region\n--s3Endpoint        \n                    AWS endpoint can be used for AWS compatible backends such as\n                    OpenStack Swift and OpenStack Ceph\n--s3SSLEnabled      \n                    Use SSL to connect to AWS [default true]\n                    \n--s3ForcePathStyle  Force path style URLs for S3 objects [default false]\n                    \n--s3Compress\n                    gzip data before sending to s3  \n--s3ServerSideEncryption\n                    Enables encrypted uploads\n--s3SSEKMSKeyId\n                    KMS Id to be used with aws:kms uploads                    \n--s3ACL\n                    S3 ACL: private | public-read | public-read-write | authenticated-read | aws-exec-read |\n                    bucket-owner-read | bucket-owner-full-control [default private]\n--s3StorageClass\n                    Set the Storage Class used for s3\n                    (default: STANDARD)   \n--s3Options\n                    Set all s3 parameters shown here https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/S3.html#createMultipartUpload-property\n                    A escaped JSON string or file can be supplied. File location must be prefixed with the @ symbol\n                    (default: null)\n--s3Configs\n                    Set all s3 constructor configurations\n                    A escaped JSON string or file can be supplied. File location must be prefixed with the @ symbol\n                    (default: null)\n--retryDelayBase\n                    The base number of milliseconds to use in the exponential backoff for operation retries. (s3)\n--customBackoff\n                    Activate custom customBackoff function. (s3)\n--tlsAuth\n                    Enable TLS X509 client authentication\n--cert, --input-cert, --output-cert\n                    Client certificate file. Use --cert if source and destination are identical.\n                    Otherwise, use the one prefixed with --input or --output as needed.\n--key, --input-key, --output-key\n                    Private key file. Use --key if source and destination are identical.\n                    Otherwise, use the one prefixed with --input or --output as needed.\n--pass, --input-pass, --output-pass\n                    Pass phrase for the private key. Use --pass if source and destination are identical.\n                    Otherwise, use the one prefixed with --input or --output as needed.\n--ca, --input-ca, --output-ca\n                    CA certificate. Use --ca if source and destination are identical.\n                    Otherwise, use the one prefixed with --input or --output as needed.\n--inputSocksProxy, --outputSocksProxy\n                    Socks5 host address\n--inputSocksPort, --outputSocksPort\n                    Socks5 host port\n--handleVersion\n                    Tells elastisearch transport to handle the `_version` field if present in the dataset\n                    (default : false)\n--versionType\n                    Elasticsearch versioning types. Should be `internal`, `external`, `external_gte`, `force`.\n                    NB : Type validation is handled by the bulk endpoint and not by elasticsearch-dump\n--csvDelimiter        \n                    The delimiter that will separate columns.\n                    (default : ',')\n--csvFirstRowAsHeaders        \n                    If set to true the first row will be treated as the headers.\n                    (default : true)\n--csvRenameHeaders        \n                    If you want the first line of the file to be removed and replaced by the one provided in the `csvCustomHeaders` option\n                    (default : true)\n--csvCustomHeaders  A comma-seperated listed of values that will be used as headers for your data. This param must\n                    be used in conjunction with `csvRenameHeaders`\n                    (default : null)\n--csvWriteHeaders   Determines if headers should be written to the csv file.\n                    (default : true)\n--csvIgnoreEmpty        \n                    Set to true to ignore empty rows. \n                    (default : false)\n--csvSkipLines        \n                    If number is \u003e 0 the specified number of lines will be skipped.\n                    (default : 0)\n--csvSkipRows        \n                    If number is \u003e 0 then the specified number of parsed rows will be skipped\n                    NB:  (If the first row is treated as headers, they aren't a part of the count)\n                    (default : 0)\n--csvMaxRows        \n                    If number is \u003e 0 then only the specified number of rows will be parsed.(e.g. 100 would return the first 100 rows of data)\n                    (default : 0)\n--csvTrim        \n                    Set to true to trim all white space from columns.\n                    (default : false)\n--csvRTrim        \n                    Set to true to right trim all columns.\n                    (default : false)\n--csvLTrim        \n                    Set to true to left trim all columns.\n                    (default : false)   \n--csvHandleNestedData        \n                    Set to true to handle nested JSON/CSV data. \n                    NB : This is a very optioninated implementaton !\n                    (default : false)\n--csvIdColumn        \n                    Name of the column to extract the record identifier (id) from\n                    When exporting to CSV this column can be used to override the default id (@id) column name\n                    (default : null)   \n--csvIndexColumn        \n                    Name of the column to extract the record index from\n                    When exporting to CSV this column can be used to override the default index (@index) column name\n                    (default : null)\n--csvTypeColumn        \n                    Name of the column to extract the record type from\n                    When exporting to CSV this column can be used to override the default type (@type) column name\n                    (default : null)   \n--force-os-version   \n                    Forces the OpenSearch version used by elasticsearch-dump.\n                    (default: 7.10.2)                       \n--help\n                    This page   \n"},{"Type":"NodeCodeBlockFenceCloseMarker","Data":"```"}]},{"ID":"20230724220446-1a7tixo","Type":"NodeParagraph","Properties":{"id":"20230724220446-1a7tixo","updated":"20230724220446"}}]}