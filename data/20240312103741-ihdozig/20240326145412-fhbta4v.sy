{"ID":"20240326145412-fhbta4v","Spec":"1","Type":"NodeDocument","Properties":{"icon":"2696-fe0f","id":"20240326145412-fhbta4v","title":"vLLM框架原理——PagedAttention","updated":"20240326150557"},"Children":[{"ID":"20240326145515-n48ska9","Type":"NodeHeading","HeadingLevel":1,"Properties":{"id":"20240326145515-n48ska9","updated":"20240326145521"},"Children":[{"Type":"NodeText","Data":"简介"}]},{"ID":"20240326145523-o83ekfv","Type":"NodeParagraph","Properties":{"id":"20240326145523-o83ekfv","updated":"20240326145546"},"Children":[{"Type":"NodeText","Data":"vLLM 是一个开源的大模型推理加速框架，通过PagedAttention高效地管理attention中缓存的张量，实现了比HuggingFace Transformers高24倍的吞吐量。"}]},{"ID":"20240326145547-aricllq","Type":"NodeHeading","HeadingLevel":1,"Properties":{"id":"20240326145547-aricllq","updated":"20240326145603"},"Children":[{"Type":"NodeText","Data":"PagedAttention"}]},{"ID":"20240326145605-ap29qk9","Type":"NodeParagraph","Properties":{"id":"20240326145605-ap29qk9","updated":"20240326145612"},"Children":[{"Type":"NodeText","Data":"作者发现大模型推理的性能瓶颈主要来自于内存。一是自回归过程中缓存的K和V张量非常大，在LLaMA-13B中，单个序列输入进来需要占用1.7GB内存。二是内存占用是动态的，取决于输入序列的长度。由于碎片化和过度预留，现有的系统浪费了60%-80%的内存。"}]},{"ID":"20240326145412-9mynsfg","Type":"NodeParagraph","Properties":{"id":"20240326145412-9mynsfg","updated":"20240326145709"},"Children":[{"Type":"NodeText","Data":"PagedAttention 灵感来自于操作系统中"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"虚拟内存"},{"Type":"NodeText","Data":"和"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"分页"},{"Type":"NodeText","Data":"的经典思想，它可以允许在非连续空间立存储连续的KV张量。具体来说，"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"PagedAttention把每个序列的KV缓存进行了分块，每个块包含固定长度的token"},{"Type":"NodeText","Data":"，而在计算attention时可以高效地找到并获取那些块。"}]},{"ID":"20240326145919-nfxwnm9","Type":"NodeParagraph","Properties":{"id":"20240326145919-nfxwnm9","updated":"20240326145928"},"Children":[{"Type":"NodeText","Data":"kv 缓存分片的过程："}]},{"ID":"20240326145738-hf1swac","Type":"NodeParagraph","Properties":{"id":"20240326145738-hf1swac","updated":"20240326145753"},"Children":[{"Type":"NodeText","Data":"​​"}]},{"ID":"20240326145826-8wyabvd","Type":"NodeParagraph","Properties":{"id":"20240326145826-8wyabvd","updated":"20240326145826"},"Children":[{"Type":"NodeText","Data":"​"},{"Type":"NodeImage","Data":"span","Children":[{"Type":"NodeBang"},{"Type":"NodeOpenBracket"},{"Type":"NodeLinkText","Data":"v2-e8a2317d1bc7ba5670ca05f68196453e_b"},{"Type":"NodeCloseBracket"},{"Type":"NodeOpenParen"},{"Type":"NodeLinkDest","Data":"assets/v2-e8a2317d1bc7ba5670ca05f68196453e_b-20240326145826-uytd6sn.webp"},{"Type":"NodeCloseParen"}]},{"Type":"NodeText","Data":"​"}]},{"ID":"20240326145831-9aq4xqg","Type":"NodeParagraph","Properties":{"id":"20240326145831-9aq4xqg","updated":"20240326145852"},"Children":[{"Type":"NodeText","Data":"每个固定长度的块可以看成虚拟内存中的页，token可以看成字节，序列可以看成进程。那么通过一个块表就可以将连续的逻辑块映射到非连续的物理块，而物理块可以根据新生成的token按需分配。"}]},{"ID":"20240326145930-41dzovz","Type":"NodeParagraph","Properties":{"id":"20240326145930-41dzovz","updated":"20240326145946"},"Children":[{"Type":"NodeText","Data":"通过 PagedAttention生成序列的过程；"}]},{"ID":"20240326150001-vw2nbe5","Type":"NodeParagraph","Properties":{"id":"20240326150001-vw2nbe5","updated":"20240326150001"},"Children":[{"Type":"NodeText","Data":"​"},{"Type":"NodeImage","Data":"span","Children":[{"Type":"NodeBang"},{"Type":"NodeOpenBracket"},{"Type":"NodeLinkText","Data":"v2-6035b0440dd9f0eb37bc9c221b977799_b"},{"Type":"NodeCloseBracket"},{"Type":"NodeOpenParen"},{"Type":"NodeLinkDest","Data":"assets/v2-6035b0440dd9f0eb37bc9c221b977799_b-20240326150001-rqc3m08.webp"},{"Type":"NodeCloseParen"}]},{"Type":"NodeText","Data":"​"}]},{"ID":"20240326150137-8qumdp7","Type":"NodeParagraph","Properties":{"id":"20240326150137-8qumdp7","updated":"20240326150145"},"Children":[{"Type":"NodeText","Data":"所以序列在分块之后，只有最后一个块可能会浪费内存（实际中浪费的内存低于4%）。高效利用内存的好处很明显：系统可以在一个batch中同时输入更多的序列，提升GPU的利用率，显著地提升吞吐量。"}]},{"ID":"20240326150150-yw0b80s","Type":"NodeParagraph","Properties":{"id":"20240326150150-yw0b80s","updated":"20240326150157"},"Children":[{"Type":"NodeText","Data":"PagedAttention的另外一个好处是"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"高效内存共享"},{"Type":"NodeText","Data":"。例如，在并行采样的时候，一个prompt需要生成多个输出序列。这种情况下，对于这个prompt的计算和内存可以在输出序列之间共享。"}]},{"ID":"20240326150220-22rgk1j","Type":"NodeParagraph","Properties":{"id":"20240326150220-22rgk1j","updated":"20240326150229"},"Children":[{"Type":"NodeText","Data":"并行采样的例子："}]},{"ID":"20240326150210-lzv8x15","Type":"NodeParagraph","Properties":{"id":"20240326150210-lzv8x15","updated":"20240326150210"},"Children":[{"Type":"NodeText","Data":"​"},{"Type":"NodeImage","Data":"span","Children":[{"Type":"NodeBang"},{"Type":"NodeOpenBracket"},{"Type":"NodeLinkText","Data":"v2-b22751a85181c355d4acaa222d781afe_b"},{"Type":"NodeCloseBracket"},{"Type":"NodeOpenParen"},{"Type":"NodeLinkDest","Data":"assets/v2-b22751a85181c355d4acaa222d781afe_b-20240326150210-2qf2suq.webp"},{"Type":"NodeCloseParen"}]},{"Type":"NodeText","Data":"​"}]},{"ID":"20240326150211-mjwe47j","Type":"NodeParagraph","Properties":{"id":"20240326150211-mjwe47j","updated":"20240326150240"},"Children":[{"Type":"NodeText","Data":"通过块表可以自然地实现内存共享。类似进程之间共享物理页，在PagedAttention中的不同序列通过将逻辑块映射到一样的物理块上可以实现共享块。为了确保安全共享，PagedAttention跟踪物理块的引用计数，并实现了"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"Copy-on-Write"},{"Type":"NodeText","Data":"机制。 内存共享减少了55%内存使用量，大大降低了采样算法的内存开销，同时提升了高达2.2倍的吞吐量。"}]},{"ID":"20240326150311-5mr5jst","Type":"NodeParagraph","Properties":{"id":"20240326150311-5mr5jst","updated":"20240326150311"},"Children":[{"Type":"NodeText","Data":"​"},{"Type":"NodeImage","Data":"span","Children":[{"Type":"NodeBang"},{"Type":"NodeOpenBracket"},{"Type":"NodeLinkText","Data":"v2-cab043f5f4d3ed2f4e369a542617fb22_b"},{"Type":"NodeCloseBracket"},{"Type":"NodeOpenParen"},{"Type":"NodeLinkDest","Data":"assets/v2-cab043f5f4d3ed2f4e369a542617fb22_b-20240326150311-9c405sf.webp"},{"Type":"NodeCloseParen"}]},{"Type":"NodeText","Data":"​"}]},{"ID":"20240326150557-e7mkf44","Type":"NodeParagraph","Properties":{"id":"20240326150557-e7mkf44"}}]}